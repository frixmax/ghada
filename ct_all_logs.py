#!/usr/bin/env python3
"""
Monitoring Certificate Transparency - VERSION Z√âRO PERTE
Optimis√© pour traiter TOUS les certificats sans en manquer
"""

import requests
import json
import time
import os
import threading
import base64
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
from cryptography import x509
from cryptography.hazmat.backends import default_backend
from concurrent.futures import ThreadPoolExecutor, as_completed

print("=" * 80)
print("MONITORING CT - VERSION Z√âRO PERTE")
print(f"Date: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
print("=" * 80)

# ==================== CONFIGURATION ====================
PORT = int(os.environ.get('PORT', 10000))
DISCORD_WEBHOOK = os.environ.get('DISCORD_WEBHOOK', 
    "https://discord.com/api/webhooks/1471764024797433872/WpHl_7qk5u9mocNYd2LbnFBp0qXbff3RXAIsrKVNXspSQJHJOp_e4_XhWOaq4jrSjKtS")
DOMAINS_FILE = '/app/domains.txt'

# OPTIMISATIONS ANTI-PERTE
CHECK_INTERVAL = 90              # 1.5 minutes (√©quilibre rapidit√©/charge)
BATCH_SIZE = 200                 # Taille des batchs
MAX_BATCHES_PER_CYCLE = 10       # Max 10 batches par log par cycle = 2000 certs
PARALLEL_LOGS = 5                # Traiter 5 logs en parall√®le
CACHE_MAX_SIZE = 100000          # Cache large pour √©viter duplicates
BACKLOG_WARNING_THRESHOLD = 1000 # Alerte si retard > 1000 certs

# LOGS ACTIFS V√âRIFI√âS - Tri√©s par activit√©
CT_LOGS = [
    # === TOP 3 - ULTRA ACTIFS (3+ milliards) ===
    {"name": "Cloudflare Nimbus2025", "url": "https://ct.cloudflare.com/logs/nimbus2025", "enabled": True, "priority": "HIGH"},
    {"name": "Cloudflare Nimbus2026", "url": "https://ct.cloudflare.com/logs/nimbus2026", "enabled": True, "priority": "HIGH"},
    {"name": "Google Argon2025h2", "url": "https://ct.googleapis.com/logs/us1/argon2025h2", "enabled": True, "priority": "HIGH"},
    
    # === TR√àS ACTIFS (2+ milliards) ===
    {"name": "Google Argon2024", "url": "https://ct.googleapis.com/logs/us1/argon2024", "enabled": True, "priority": "HIGH"},
    {"name": "Google Argon2026h1", "url": "https://ct.googleapis.com/logs/us1/argon2026h1", "enabled": True, "priority": "HIGH"},
    
    # === ACTIFS (1+ milliard) ===
    {"name": "Google Argon2025h1", "url": "https://ct.googleapis.com/logs/us1/argon2025h1", "enabled": True, "priority": "MEDIUM"},
    
    # === MOYENNEMENT ACTIFS ===
    {"name": "Google Argon2026h2", "url": "https://ct.googleapis.com/logs/us1/argon2026h2", "enabled": True, "priority": "MEDIUM"},
    {"name": "Cloudflare Nimbus2027", "url": "https://ct.cloudflare.com/logs/nimbus2027", "enabled": True, "priority": "MEDIUM"},
    {"name": "Google Solera2026h1", "url": "https://ct.googleapis.com/logs/eu1/solera2026h1", "enabled": True, "priority": "MEDIUM"},
    {"name": "Google Solera2024", "url": "https://ct.googleapis.com/logs/eu1/solera2024", "enabled": True, "priority": "LOW"},
    
    # === MOINS ACTIFS ===
    {"name": "Google Solera2025h2", "url": "https://ct.googleapis.com/logs/eu1/solera2025h2", "enabled": True, "priority": "LOW"},
    {"name": "Google Solera2025h1", "url": "https://ct.googleapis.com/logs/eu1/solera2025h1", "enabled": True, "priority": "LOW"},
    {"name": "Google Solera2026h2", "url": "https://ct.googleapis.com/logs/eu1/solera2026h2", "enabled": True, "priority": "LOW"},
]

# ==================== STATS ====================
stats = {
    'certificats_analys√©s': 0,
    'alertes_envoy√©es': 0,
    'derni√®re_alerte': None,
    'd√©marrage': datetime.utcnow(),
    'derni√®re_v√©rification': None,
    'positions': {},
    'logs_actifs': 0,
    'logs_en_erreur': {},
    'duplicates_√©vit√©s': 0,
    'parse_errors': 0,
    'matches_trouv√©s': 0,
    'total_backlog': 0,
    'batches_processed': 0,
    'max_backlog_seen': 0,
}

# Cache anti-duplicates
seen_certificates = set()

# Lock pour thread-safety
stats_lock = threading.Lock()

# ==================== CHARGEMENT DOMAINES ====================
try:
    with open(DOMAINS_FILE, 'r') as f:
        targets = {line.strip().lower() for line in f if line.strip() and not line.startswith('#')}
    print(f"‚úì {len(targets)} domaines charg√©s")
    if targets:
        print(f"  Premiers: {', '.join(list(targets)[:5])}")
except Exception as e:
    print(f"‚úó Erreur chargement domaines: {e}")
    targets = set()

if not targets:
    print("‚úó ERREUR: Aucun domaine √† surveiller")
    exit(1)

if "discord.com/api/webhooks" not in DISCORD_WEBHOOK:
    print("‚úó ERREUR: Webhook Discord invalide")
    exit(1)

stats['logs_actifs'] = sum(1 for log in CT_LOGS if log['enabled'])
print(f"‚úì {stats['logs_actifs']} logs CT actifs")
print(f"‚úì Webhook Discord configur√©")
print(f"‚úì Anti-perte: {MAX_BATCHES_PER_CYCLE} batches/cycle √ó {BATCH_SIZE} = {MAX_BATCHES_PER_CYCLE * BATCH_SIZE} certs max/log/cycle")
print("=" * 80)

# ==================== HTTP HEALTH CHECK ====================
class HealthCheckHandler(BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        pass
    
    def do_GET(self):
        if self.path == '/health' or self.path == '/':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            
            uptime = (datetime.utcnow() - stats['d√©marrage']).total_seconds()
            
            status = {
                'status': 'healthy',
                'uptime_seconds': int(uptime),
                'uptime_human': f"{int(uptime//3600)}h {int((uptime%3600)//60)}m",
                'certificats_analys√©s': stats['certificats_analys√©s'],
                'matches_trouv√©s': stats['matches_trouv√©s'],
                'alertes_envoy√©es': stats['alertes_envoy√©es'],
                'duplicates_√©vit√©s': stats['duplicates_√©vit√©s'],
                'parse_errors': stats['parse_errors'],
                'batches_processed': stats['batches_processed'],
                'total_backlog': stats['total_backlog'],
                'max_backlog_seen': stats['max_backlog_seen'],
                'logs_actifs': stats['logs_actifs'],
                'logs_en_erreur': stats['logs_en_erreur'],
                'derni√®re_alerte': stats['derni√®re_alerte'].isoformat() if stats['derni√®re_alerte'] else None,
                'derni√®re_v√©rification': stats['derni√®re_v√©rification'].isoformat() if stats['derni√®re_v√©rification'] else None,
                'timestamp': datetime.utcnow().isoformat(),
                'logs_positions': stats['positions']
            }
            
            self.wfile.write(json.dumps(status, indent=2).encode())
        else:
            self.send_response(404)
            self.end_headers()

def start_http_server():
    server = HTTPServer(('0.0.0.0', PORT), HealthCheckHandler)
    print(f"‚úì Serveur HTTP sur port {PORT}")
    server.serve_forever()

# ==================== CT API FUNCTIONS ====================
def get_sth(log_url, log_name):
    """R√©cup√®re le Signed Tree Head"""
    try:
        response = requests.get(f"{log_url}/ct/v1/get-sth", timeout=10)
        response.raise_for_status()
        
        with stats_lock:
            if log_name in stats['logs_en_erreur']:
                del stats['logs_en_erreur'][log_name]
        
        return response.json()['tree_size']
    except Exception as e:
        with stats_lock:
            stats['logs_en_erreur'][log_name] = str(e)[:100]
        return None

def get_entries(log_url, start, end):
    """R√©cup√®re les entr√©es CT avec retry"""
    for attempt in range(3):  # 3 tentatives
        try:
            response = requests.get(
                f"{log_url}/ct/v1/get-entries",
                params={"start": start, "end": end},
                timeout=30
            )
            response.raise_for_status()
            return response.json().get('entries', [])
        except Exception as e:
            if attempt == 2:  # Derni√®re tentative
                print(f"    ‚úó Erreur get_entries apr√®s 3 tentatives: {str(e)[:50]}")
                return []
            time.sleep(1)  # Pause avant retry
    return []

def generate_cert_hash(entry):
    """G√©n√®re hash pour d√©tecter duplicates"""
    try:
        return hash(entry.get('leaf_input', ''))
    except:
        return None

def parse_certificate(entry):
    """Parse certificat X.509 - OPTIMIS√â"""
    try:
        leaf_bytes = base64.b64decode(entry.get('leaf_input', ''))
        
        if len(leaf_bytes) < 15:
            return []
        
        # Extract certificate from Merkle Tree Leaf
        cert_length = int.from_bytes(leaf_bytes[11:14], 'big')
        cert_start = 14
        cert_end = cert_start + cert_length
        
        if cert_end > len(leaf_bytes):
            return []
        
        cert_der = leaf_bytes[cert_start:cert_end]
        cert = x509.load_der_x509_certificate(cert_der, default_backend())
        
        all_domains = set()
        
        # Common Name
        try:
            cn = cert.subject.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)
            if cn:
                all_domains.add(cn[0].value.lower())
        except:
            pass
        
        # SANs (Subject Alternative Names)
        try:
            san_ext = cert.extensions.get_extension_for_oid(
                x509.oid.ExtensionOID.SUBJECT_ALTERNATIVE_NAME
            )
            for san in san_ext.value:
                domain = san.value.lower().lstrip('*.')
                all_domains.add(domain)
        except:
            pass
        
        # Match avec targets
        matched = []
        for domain in all_domains:
            for target in targets:
                if domain == target or domain.endswith('.' + target):
                    matched.append(domain)
                    break
        
        return list(set(matched))
        
    except Exception as e:
        with stats_lock:
            stats['parse_errors'] += 1
        return []

def send_alert(matched_domains, log_name):
    """Envoie alerte Discord - Rate limited"""
    try:
        unique_domains = sorted(set(matched_domains))
        
        # Grouper par domaine principal
        by_base_domain = {}
        for domain in unique_domains:
            for target in targets:
                if domain == target or domain.endswith('.' + target):
                    if target not in by_base_domain:
                        by_base_domain[target] = []
                    by_base_domain[target].append(domain)
                    break
        
        # Construire description
        description_parts = []
        for base, subs in sorted(by_base_domain.items()):
            description_parts.append(f"**{base}** ({len(subs)})")
            for sub in sorted(subs)[:5]:  # Max 5 par domaine
                description_parts.append(f"  ‚Ä¢ `{sub}`")
            if len(subs) > 5:
                description_parts.append(f"  ... +{len(subs)-5} autres")
        
        description = "\n".join(description_parts[:30])  # Max 30 lignes
        
        if len(description_parts) > 30:
            description += f"\n\n... et {len(description_parts)-30} lignes suppl√©mentaires"
        
        embed = {
            "title": "üö® Nouveau(x) certificat(s) SSL",
            "description": description,
            "color": 0xff0000,
            "fields": [
                {"name": "Total domaines", "value": str(len(unique_domains)), "inline": True},
                {"name": "Domaines principaux", "value": str(len(by_base_domain)), "inline": True},
                {"name": "Source", "value": log_name, "inline": True}
            ],
            "footer": {"text": "CT Monitor - Z√©ro Perte"},
            "timestamp": datetime.utcnow().isoformat()
        }
        
        payload = {"embeds": [embed]}
        response = requests.post(DISCORD_WEBHOOK, json=payload, timeout=10)
        response.raise_for_status()
        
        with stats_lock:
            stats['alertes_envoy√©es'] += 1
            stats['derni√®re_alerte'] = datetime.utcnow()
        
        print(f"    ‚úì Alerte Discord: {len(unique_domains)} domaine(s)")
        
    except Exception as e:
        print(f"    ‚úó Erreur Discord: {str(e)[:100]}")

# ==================== MONITORING CORE - Z√âRO PERTE ====================
def monitor_log(log_config):
    """Surveille un log CT - VERSION Z√âRO PERTE"""
    log_name = log_config['name']
    log_url = log_config['url']
    priority = log_config.get('priority', 'MEDIUM')
    
    # Ajuster max_batches selon priorit√©
    if priority == 'HIGH':
        max_batches = MAX_BATCHES_PER_CYCLE
    elif priority == 'MEDIUM':
        max_batches = MAX_BATCHES_PER_CYCLE // 2
    else:
        max_batches = MAX_BATCHES_PER_CYCLE // 4
    
    # Init position
    if log_name not in stats['positions']:
        tree_size = get_sth(log_url, log_name)
        if tree_size:
            # D√©marrer 1000 entr√©es avant la fin
            stats['positions'][log_name] = max(0, tree_size - 1000)
            print(f"  ‚úì Init {log_name}: position {stats['positions'][log_name]:,} / {tree_size:,}")
        else:
            print(f"  ‚úó {log_name}: Impossible de r√©cup√©rer tree_size")
            return
    
    # R√©cup√©rer taille actuelle
    tree_size = get_sth(log_url, log_name)
    if not tree_size:
        print(f"  ‚úó {log_name}: get_sth failed")
        return
    
    current_pos = stats['positions'][log_name]
    
    if current_pos >= tree_size:
        print(f"  ‚úì {log_name}: √Ä jour ({current_pos:,} / {tree_size:,})")
        return
    
    # Calculer le retard (backlog)
    backlog = tree_size - current_pos
    
    with stats_lock:
        stats['total_backlog'] += backlog
        if backlog > stats['max_backlog_seen']:
            stats['max_backlog_seen'] = backlog
    
    # Warning si retard important
    if backlog > BACKLOG_WARNING_THRESHOLD:
        print(f"  ‚ö†Ô∏è  {log_name}: RETARD de {backlog:,} certificats ! (Traitement de {max_batches} batches)")
    
    # TRAITER TOUS LES CERTIFICATS EN RETARD (par batches)
    batches_processed = 0
    total_matches = []
    
    while current_pos < tree_size and batches_processed < max_batches:
        end_pos = min(current_pos + BATCH_SIZE, tree_size)
        remaining = tree_size - end_pos
        
        print(f"  üîç {log_name} [{batches_processed+1}/{max_batches}]: {current_pos:,} ‚Üí {end_pos-1:,} (reste: {remaining:,})")
        
        # R√©cup√©rer entr√©es
        entries = get_entries(log_url, current_pos, end_pos - 1)
        
        if not entries:
            print(f"    ‚ö†Ô∏è  Aucune entr√©e, skip batch")
            break
        
        # Parser chaque certificat
        for entry in entries:
            with stats_lock:
                stats['certificats_analys√©s'] += 1
            
            # Check duplicate
            cert_hash = generate_cert_hash(entry)
            if cert_hash:
                if cert_hash in seen_certificates:
                    with stats_lock:
                        stats['duplicates_√©vit√©s'] += 1
                    continue
                
                seen_certificates.add(cert_hash)
                
                # Limiter taille cache
                if len(seen_certificates) > CACHE_MAX_SIZE:
                    seen_certificates.pop()
            
            # Parser
            matched = parse_certificate(entry)
            
            if matched:
                with stats_lock:
                    stats['matches_trouv√©s'] += len(matched)
                total_matches.extend(matched)
        
        # Mettre √† jour position
        current_pos = end_pos
        stats['positions'][log_name] = current_pos
        batches_processed += 1
        
        with stats_lock:
            stats['batches_processed'] += 1
        
        # Petite pause entre batches (√©viter surcharge)
        time.sleep(0.3)
    
    # Envoyer alerte group√©e si matches
    if total_matches:
        print(f"  üéØ {log_name}: {len(total_matches)} match(es) trouv√©(s)")
        send_alert(total_matches, log_name)
    
    # Stats finales
    final_backlog = tree_size - current_pos
    if final_backlog > 0:
        print(f"  üìä {log_name}: {batches_processed} batches trait√©s, {final_backlog:,} certs restants")
    
    return batches_processed

# ==================== MAIN LOOP ====================
def monitor_all_logs_parallel():
    """Traite tous les logs en parall√®le"""
    enabled_logs = [log for log in CT_LOGS if log['enabled']]
    
    results = {}
    
    with ThreadPoolExecutor(max_workers=PARALLEL_LOGS) as executor:
        futures = {executor.submit(monitor_log, log): log['name'] for log in enabled_logs}
        
        for future in as_completed(futures):
            log_name = futures[future]
            try:
                batches = future.result(timeout=120)  # Max 2 min par log
                results[log_name] = batches if batches else 0
            except Exception as e:
                print(f"  ‚úó {log_name}: Exception - {str(e)[:100]}")
                results[log_name] = -1
    
    return results

# ==================== STARTUP ====================
# D√©marrer HTTP server
http_thread = threading.Thread(target=start_http_server, daemon=True)
http_thread.start()
time.sleep(1)

print(f"\nüîÑ D√©marrage surveillance (intervalle: {CHECK_INTERVAL}s)")
print(f"üéØ {len(targets)} domaine(s) surveill√©(s)")
print(f"‚ö° Parall√©lisation: {PARALLEL_LOGS} logs simultan√©s")
print("=" * 80)

# ==================== MAIN LOOP ====================
cycle = 0

while True:
    try:
        cycle += 1
        cycle_start = time.time()
        
        with stats_lock:
            stats['derni√®re_v√©rification'] = datetime.utcnow()
            stats['total_backlog'] = 0
        
        print(f"\n{'='*80}")
        print(f"CYCLE #{cycle} - {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
        print(f"{'='*80}")
        
        # Traiter tous les logs en parall√®le
        results = monitor_all_logs_parallel()
        
        # Stats du cycle
        cycle_duration = int(time.time() - cycle_start)
        successful_logs = sum(1 for v in results.values() if v >= 0)
        total_batches_this_cycle = sum(v for v in results.values() if v > 0)
        
        print(f"\n{'='*80}")
        print(f"CYCLE #{cycle} TERMIN√â en {cycle_duration}s")
        print(f"  Logs trait√©s: {successful_logs}/{len(results)}")
        print(f"  Batches trait√©s: {total_batches_this_cycle}")
        print(f"  Certificats analys√©s (total): {stats['certificats_analys√©s']:,}")
        print(f"  Matches trouv√©s (total): {stats['matches_trouv√©s']}")
        print(f"  Alertes envoy√©es (total): {stats['alertes_envoy√©es']}")
        print(f"  Backlog actuel: {stats['total_backlog']:,}")
        print(f"  Max backlog vu: {stats['max_backlog_seen']:,}")
        print(f"{'='*80}")
        
        # Attendre avant prochain cycle
        print(f"\nüí§ Attente {CHECK_INTERVAL}s avant cycle #{cycle+1}...")
        time.sleep(CHECK_INTERVAL)
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Arr√™t demand√© par l'utilisateur")
        break
    except Exception as e:
        print(f"\n‚úó ERREUR GLOBALE: {e}")
        import traceback
        traceback.print_exc()
        print("\n‚è∏Ô∏è  Pause 60s avant retry...")
        time.sleep(60)

print("\n" + "="*80)
print("ARR√äT DU MONITORING")
print(f"Uptime: {int((datetime.utcnow() - stats['d√©marrage']).total_seconds() / 3600)}h")
print(f"Certificats analys√©s: {stats['certificats_analys√©s']:,}")
print(f"Alertes envoy√©es: {stats['alertes_envoy√©es']}")
print("="*80)
